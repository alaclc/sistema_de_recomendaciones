# -*- coding: utf-8 -*-
"""Sprint 3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EazXAYiOaNpCCoWmRv4jRAGcPJX1Kl6Q

**Spring 3:** Alexis La Cruz
"""

import pandas as pd
from google.colab import drive
drive.mount('/content/drive')

# se importan las bases de datos
u2 = pd.read_table('/content/drive/MyDrive/Colab Notebooks/Data Science/Springs/Tercer Spring/archivos/u2.base')
ua = pd.read_table('/content/drive/MyDrive/Colab Notebooks/Data Science/Springs/Tercer Spring/archivos/ua.base')
ub = pd.read_table('/content/drive/MyDrive/Colab Notebooks/Data Science/Springs/Tercer Spring/archivos/ub.base')
u3 = pd.read_table('/content/drive/MyDrive/Colab Notebooks/Data Science/Springs/Tercer Spring/archivos/u3.base')
u4 = pd.read_table('/content/drive/MyDrive/Colab Notebooks/Data Science/Springs/Tercer Spring/archivos/u4.base')
u5 = pd.read_table('/content/drive/MyDrive/Colab Notebooks/Data Science/Springs/Tercer Spring/archivos/u5.base')
udata = pd.read_table('/content/drive/MyDrive/Colab Notebooks/Data Science/Springs/Tercer Spring/archivos/u.data')
u2t = pd.read_table('/content/drive/MyDrive/Colab Notebooks/Data Science/Springs/Tercer Spring/archivos/u2.test',)
uat = pd.read_table('/content/drive/MyDrive/Colab Notebooks/Data Science/Springs/Tercer Spring/archivos/ua.test')
ubt = pd.read_table('/content/drive/MyDrive/Colab Notebooks/Data Science/Springs/Tercer Spring/archivos/ub.test')
u3t = pd.read_table('/content/drive/MyDrive/Colab Notebooks/Data Science/Springs/Tercer Spring/archivos/u3.test')
u4t = pd.read_table('/content/drive/MyDrive/Colab Notebooks/Data Science/Springs/Tercer Spring/archivos/u4.test')
u5t = pd.read_table('/content/drive/MyDrive/Colab Notebooks/Data Science/Springs/Tercer Spring/archivos/u5.test')
genre = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data Science/Springs/Tercer Spring/archivos/u.genre', sep='|', encoding='latin-1')
occupation = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data Science/Springs/Tercer Spring/archivos/u.occupation', sep='|', encoding='latin-1')
user = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data Science/Springs/Tercer Spring/archivos/u.user', sep='|', encoding='latin-1')
item = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data Science/Springs/Tercer Spring/archivos/u.item', sep='|', encoding='latin-1')

print(u2.shape)
print(ua.shape)
print(ub.shape)
print(u3.shape)
print(u4.shape)
print(u5.shape)

"""Las bases de datos del train tienen 4 columnas. 4 de las bases de datos tienen 79999 filas y 2 90569 filas."""

print(u2t.shape)
print(uat.shape)
print(ubt.shape)
print(u3t.shape)
print(u4t.shape)
print(u5t.shape)

"""Las bases de datos del test tienen 4 columnas. 4 de las bases de datos tienen 19999 filas y 2 9429 filas."""

# se unen las bases de datos del train
df_train = pd.concat([ua,ub,u2,u3,u4,u5],ignore_index=True)

# se unen las bases de datos para generar el dataset de validación
df_test = pd.concat([uat,ubt,u2t,u3t,u4t,u5t],ignore_index=True)

df_train = df_train.drop(['3','4','878542960'],axis=1)
df_test = df_test.drop(['17','3','875073198','1.1','5','874965758','889751712','876893119','878542960'],axis=1)

df_train.columns = ['user_id', 'item_id', 'rating', 'timestamp']
df_test.columns = ['user_id', 'item_id', 'rating', 'timestamp']

df_train.isnull().sum()/len(df_train)

df_test.isnull().sum()/len(df_test)

# se suprimen los valores nulos surgidos de la combinación de los datasets
df_train = df_train.dropna()
df_test = df_test.dropna()

# a continuación se genera un tercer dataset con datos aleatorios sustraídos del train y del test
df_val1 = df_train.sample(n=10000)
df_val2 = df_test.sample(n=1000)
df_val = pd.concat([df_val1,df_val2],ignore_index=True)

# se cambia el formato de los valores de la columna "timestamp" a fecha
from datetime import datetime
df_train["timestamp"] = df_train["timestamp"].apply(lambda x: datetime.utcfromtimestamp(x).strftime('%Y-%m-%d'))
df_val["timestamp"] = df_val["timestamp"].apply(lambda x: datetime.utcfromtimestamp(x).strftime('%Y-%m-%d'))
df_test["timestamp"] = df_test["timestamp"].apply(lambda x: datetime.utcfromtimestamp(x).strftime('%Y-%m-%d'))

# se procede a construir la matriz de interacciones
columnas = ["user_id", "item_id", "rating"]
interacciones = df_train[columnas]

interacciones = interacciones.drop_duplicates(['user_id'])

matriz_de_interacciones = interacciones.pivot(index="user_id", columns="item_id", values="rating")

matriz_de_interacciones.head()

# se reemplazan los NaN por 0
matriz_de_interacciones = matriz_de_interacciones.fillna(0)

matriz_de_interacciones.head()

print(matriz_de_interacciones.shape)

"""La matriz de interacciones tiene 943 filas y 76 columnas"""

# se procede a transformar la matriz a csr matrix dado que esta es muy grande y dispersa
from scipy.sparse import csr_matrix
matriz_de_interacciones_csr = csr_matrix(matriz_de_interacciones.values)
matriz_de_interacciones_csr

user_ids = list(matriz_de_interacciones.index)
user_dict = {}
counter = 0 
for i in user_ids:
    user_dict[i] = counter
    counter += 1

user_dict

pip install lightfm

# se entrena el modelo
from lightfm import LightFM
modelo = LightFM(no_components=3, random_state=100, learning_rate=0.03)

modelo = modelo.fit(matriz_de_interacciones_csr, epochs=20)

n_users, n_items = matriz_de_interacciones.shape

import numpy as np
user_x = user_dict[1]
n_users, n_items = matriz_de_interacciones.shape
item_ids = np.arange(n_items)

predicciones = modelo.predict(user_ids=user_x, item_ids = item_ids)

scores = pd.Series(predicciones)
scores.index = matriz_de_interacciones.columns
recomendaciones = list(pd.Series(scores.sort_values(ascending=False).index))[:20]

"""A continuación se muestran los id de los contenidos recomendados para el usuario 1:"""

recomendaciones

"""A continuación se muestran los id de los contenidos que visualizó el usuario número 1, con rating igual al último decil."""

df_train[(df_train.user_id==1)&(df_train.rating==df_train['rating'].quantile(q=0.9)) & (df_train.item_id.isin(recomendaciones))].item_id.unique()

"""El usuario número 1 ya había visto la mayoría del contenido, sin embargo, 7 de ellos tienen un rating ubicado en el percentil 90."""

df_train[(df_train.user_id==1) & (df_train.item_id.isin(recomendaciones))].item_id.unique()

"""En conclusión, el sistema de recomendación parecer ser no muy bueno, dado que recomendó al usuario en cuestión contenido que en su mayoría él ya había visualizado."""